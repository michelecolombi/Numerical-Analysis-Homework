{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd956214-1602-4da4-b997-a3abaa704961",
   "metadata": {},
   "source": [
    "# Numerical Optmization Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e2afc3-3a33-41b9-a1e5-b2528ad476d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb05fa-d256-4d8b-9ca4-8bb396703459",
   "metadata": {},
   "source": [
    "## Exercise 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5cd1a0d-fc32-479a-919a-1c50588f5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the n×n Hilbert matrix  H_ij = 1/(i+j-1)\n",
    "def hilbert(n):\n",
    "    i = np.arange(1, n + 1)\n",
    "    return 1.0 / (i[:, None] + i[None, :] - 1.0)\n",
    "\n",
    "# Conjugate gradient method (algorithm 5.2)\n",
    "def cg(A, b, x0, tol, maxit, verbose = False):\n",
    "    \"\"\"Solve  A x = b  with CG.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : SPD matrix (n×n)\n",
    "    b : right‑hand side (n,)\n",
    "    x0 : starting point (defaults to zeros)\n",
    "    tol : stop when  ‖r_k‖ ≤ tol ‖r_0‖\n",
    "    maxit : maximum iterations \n",
    "    return_history : if True, also return list of residual norms\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x      : approximate solution\n",
    "    it     : iterations performed\n",
    "    history: list of residual norms \n",
    "    \"\"\"\n",
    "   \n",
    "    n = b.size\n",
    "    x = x0.copy()\n",
    "\n",
    "    r = A @ x - b           # r0 = A x0 – b \n",
    "    p = -r.copy()           # p0 = -r0\n",
    "    rs_old = r @ r          # r'r\n",
    "\n",
    "    res0 = np.sqrt(rs_old)\n",
    "    history: list[float] = [res0]\n",
    "\n",
    "    for k in range(1, maxit + 1):\n",
    "        alpha = rs_old / (p @ (A @ p))      # (5.24a)\n",
    "        x += alpha * p                      # (5.24b)\n",
    "        r += alpha * (A @ p)                # (5.24c)\n",
    "\n",
    "        rs_new = r @ r\n",
    "        res = np.sqrt(rs_new)\n",
    "        history.append(res)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"k {k} -- res = {res}\")\n",
    "\n",
    "        if res <= tol * res0:\n",
    "            return (x, k) \n",
    "\n",
    "        beta = rs_new / rs_old              # (5.24d)\n",
    "        p = -r + beta * p                   # (5.24e)\n",
    "        \n",
    "        rs_old = rs_new\n",
    "\n",
    "    return (x, maxit)\n",
    "\n",
    "\n",
    "# Exercise 5.1 driver\n",
    "def run():\n",
    "    dims = [5, 8, 12, 20]\n",
    "    tol = 1e-6\n",
    "    print(\"n   iterations \")\n",
    "    print(\"-   ---------- \")\n",
    "\n",
    "    for n in dims:\n",
    "        H = hilbert(n)\n",
    "        b = np.ones(n)\n",
    "        x0 = np.zeros(n)\n",
    "\n",
    "        _, it = cg(H, b, x0, tol=tol, maxit=100)\n",
    "        print(f\"{n:<3d} {it:^10d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1584840e-8d69-41b2-9a32-b80109c83c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n   iterations \n",
      "-   ---------- \n",
      "5       6     \n",
      "8       19    \n",
      "12      37    \n",
      "20      68    \n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5048486-c3ee-4366-930b-090bffe4e6ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 14.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3ce6f91-21f0-4bf7-9a58-4f3efc230c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mehrotra predictor–corrector interior‑point LP solver\n",
    "    *with every Newton step solved as the FULL KKT system* (Eq. 14.30).\n",
    "\n",
    "This file keeps everything in **one function** (`mehrotra_lp`) exactly as you\n",
    "asked – no separate helpers, no normal‑equation reduction.  In each Newton\n",
    "iteration we build the dense\n",
    "\n",
    "        ┌            ┐\n",
    "        │ 0  Aᵀ  I  │\n",
    "    K = │ A   0   0 │   (14.30)\n",
    "        │ S   0   X │\n",
    "        └            ┘\n",
    "\n",
    "and solve it with `numpy.linalg.solve` both for the *affine‑scaling* (predictor)\n",
    "step and the *combined predictor+corrector* step.\n",
    "\n",
    "At the bottom of the file you'll find a **tiny demo** that builds a random LP\n",
    "following the textbook’s recipe (your screenshot) and calls the solver.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Random LP generator (recipe from the book)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def generate_random_lp(m: int, n: int, *, seed: int | None = None):\n",
    "    \"\"\"Return (A, b, c, x⋆) where x⋆ is the known optimum.\n",
    "\n",
    "    The construction is exactly what your screenshot specifies:\n",
    "\n",
    "        x⋆_i > 0  for i = 1..m,   x⋆_i = 0  otherwise\n",
    "        s⋆_i > 0  for i = m+1..n,  s⋆_i = 0  otherwise\n",
    "        λ⋆  random,  c = Aᵀ λ⋆ + s⋆,  b = A x⋆.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    A = rng.standard_normal((m, n))\n",
    "\n",
    "    x_star = np.zeros(n)\n",
    "    x_star[:m] = rng.random(m) + 0.5  # positive\n",
    "\n",
    "    s_star = np.zeros(n)\n",
    "    s_star[m:] = rng.random(n - m) + 0.5  # positive\n",
    "\n",
    "    lam_star = rng.standard_normal(m)\n",
    "\n",
    "    c = A.T @ lam_star + s_star\n",
    "    b = A @ x_star\n",
    "\n",
    "    return A, b, c, x_star\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Mehrotra predictor–corrector IPM (ALL full KKT solves, one function)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def mehrotra_lp(\n",
    "    A: np.ndarray,\n",
    "    b: np.ndarray,\n",
    "    c: np.ndarray,\n",
    "    *,\n",
    "    max_iter: int = 50,\n",
    "    tol: float = 1e-8,\n",
    "    eta: float = 0.99,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"Solve  min cᵀx  s.t.  Ax = b,  x ≥ 0  using Algorithm 14.3.\n",
    "\n",
    "    Every Newton system – both predictor and corrector – is solved as the FULL\n",
    "    KKT system of Eq. (14.30).  Returns (x, λ, s).\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Strictly positive starting point – simple but effective.\n",
    "    x   = np.full(n, 10.0)\n",
    "    s   = np.full(n, 10.0)\n",
    "    lam = np.zeros(m)\n",
    "\n",
    "    e = np.ones(n)\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        r_b = A @ x - b\n",
    "        r_c = A.T @ lam + s - c\n",
    "        mu   = (x @ s) / n\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"iter {k:2d}  ‖r_b‖={np.linalg.norm(r_b):.1e}  ‖r_c‖={np.linalg.norm(r_c):.1e}  μ={mu:.1e}\")\n",
    "\n",
    "        if max(np.linalg.norm(r_b), np.linalg.norm(r_c), mu) < tol:\n",
    "            break\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1) Predictor (affine‑scaling) step – FULL KKT solve\n",
    "        # ------------------------------------------------------------------\n",
    "        X = np.diag(x)\n",
    "        S = np.diag(s)\n",
    "        K = np.block([\n",
    "            [np.zeros((n, n)),   A.T,               np.eye(n)],\n",
    "            [A,                  np.zeros((m, m)),  np.zeros((m, n))],\n",
    "            [S,                  np.zeros((n, m)),  X],\n",
    "        ])\n",
    "        rhs_aff = np.concatenate([-r_c, -r_b, -x * s])\n",
    "        delta_aff = np.linalg.solve(K, rhs_aff)\n",
    "        dx_aff   = delta_aff[:n]\n",
    "        dlam_aff = delta_aff[n:n + m]\n",
    "        ds_aff   = delta_aff[n + m:]\n",
    "\n",
    "        # Step lengths (14.32) ---------------------------------------------\n",
    "        alpha_aff_pri = 1.0\n",
    "        neg_dx = dx_aff < 0\n",
    "        if np.any(neg_dx):\n",
    "            alpha_aff_pri = min(1.0, np.min(-x[neg_dx] / dx_aff[neg_dx]))\n",
    "\n",
    "        alpha_aff_dua = 1.0\n",
    "        neg_ds = ds_aff < 0\n",
    "        if np.any(neg_ds):\n",
    "            alpha_aff_dua = min(1.0, np.min(-s[neg_ds] / ds_aff[neg_ds]))\n",
    "\n",
    "        mu_aff = ((x + alpha_aff_pri * dx_aff) @ (s + alpha_aff_dua * ds_aff)) / n\n",
    "        sigma  = (mu_aff / mu) ** 3  # (14.34)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2) Corrector + centering step – FULL KKT with modified RHS\n",
    "        # ------------------------------------------------------------------\n",
    "        rxs = dx_aff * ds_aff  # element‑wise\n",
    "        rhs_cor = np.concatenate([\n",
    "            -r_c,\n",
    "            -r_b,\n",
    "            -x * s - rxs + sigma * mu * e,\n",
    "        ])\n",
    "        delta = np.linalg.solve(K, rhs_cor)\n",
    "        dx   = delta[:n]\n",
    "        dlam = delta[n:n + m]\n",
    "        ds   = delta[n + m:]\n",
    "\n",
    "        # Fraction‑to‑boundary rule (14.36–14.38) --------------------------\n",
    "        alpha_pri_max = 1.0\n",
    "        neg_dx = dx < 0\n",
    "        if np.any(neg_dx):\n",
    "            alpha_pri_max = min(alpha_pri_max, np.min(-x[neg_dx] / dx[neg_dx]))\n",
    "\n",
    "        alpha_dua_max = 1.0\n",
    "        neg_ds = ds < 0\n",
    "        if np.any(neg_ds):\n",
    "            alpha_dua_max = min(alpha_dua_max, np.min(-s[neg_ds] / ds[neg_ds]))\n",
    "\n",
    "        alpha_pri = eta * alpha_pri_max\n",
    "        alpha_dua = eta * alpha_dua_max\n",
    "\n",
    "        # Update ------------------------------------------------------------\n",
    "        x   += alpha_pri * dx\n",
    "        lam += alpha_dua * dlam\n",
    "        s   += alpha_dua * ds\n",
    "\n",
    "    return x, lam, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84e6aeb2-2f31-4ac0-9b58-663e8c594b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0  ‖r_b‖=3.2e+01  ‖r_c‖=2.3e+01  μ=1.0e+02\n",
      "iter  1  ‖r_b‖=3.2e-01  ‖r_c‖=2.3e-01  μ=8.5e+00\n",
      "iter  2  ‖r_b‖=3.2e-03  ‖r_c‖=2.3e-03  μ=1.4e-01\n",
      "iter  3  ‖r_b‖=3.2e-05  ‖r_c‖=2.3e-05  μ=1.4e-03\n",
      "iter  4  ‖r_b‖=3.2e-07  ‖r_c‖=2.3e-07  μ=1.4e-05\n",
      "iter  5  ‖r_b‖=3.2e-09  ‖r_c‖=2.3e-09  μ=1.4e-07\n",
      "iter  6  ‖r_b‖=3.2e-11  ‖r_c‖=2.3e-11  μ=1.4e-09\n",
      "\n",
      "Optimal objective value: 4.480502807601511\n",
      "Distance to true x     : 2.2146301837596377e-11\n"
     ]
    }
   ],
   "source": [
    "m, n = 6, 6  \n",
    "A, b, c, x_true = generate_random_lp(m, n, seed=42)\n",
    "x_opt, lam_opt, s_opt = mehrotra_lp(A, b, c, verbose=True)\n",
    "print(\"\\nOptimal objective value:\", c @ x_opt)\n",
    "print(\"Distance to true x     :\", np.linalg.norm(x_opt - x_true))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
